<!--===========================================================================
  This is the build file for the HDP 1.3 Shim modules.
  
  See ../build-res/subfloor.xml for more details
============================================================================-->
<project name="pentaho-hadoop-shims-hdp13" basedir="." default="dist"
  xmlns:ivy="antlib:org.apache.ivy.ant" >

  <description>
    This build file is used to create the HDP1.3 Shim module for the Big Data plugin
    and is based off Subfloor (../build-res/subfloor.xml)
  </description>

  <import file="../../build-res/subfloor.xml"/>

  <property name="bin" value="bin" />
  <property name="run" value="run" />
  <property name="jobs-full" location="jobs" />
  <property name="stage" value="${bin}/stage" />
  <property name="run-lib" value="${run}/lib" />
  <property name="run-libswt" value="${run}/libswt" />

	<macrodef name="ivy-resolve">
    <attribute name="ivyfile" default="${ivyfile}" />
    <attribute name="conf" default="default" />
    <attribute name="directory"/>
    <sequential>
      <mkdir dir="@{directory}" />
      <ivy:resolve file="@{ivyfile}" conf="@{conf}" />
      <ivy:retrieve conf="@{conf}" pattern="@{directory}/[module]-[revision](-[classifier]).[ext]" />
    </sequential>
  </macrodef>

  <target name="resolve" depends="install-ivy,subfloor.resolve">
    <mkdir dir="${stage}/pdi-ce" />
    <mkdir dir="${run}" />
    <ivy-resolve conf="pdi-ce" directory="${stage}/pdi-ce" />
    <ivy-resolve conf="big-data-plugin" directory="${stage}/big-data-plugin" />
    <ivy-resolve conf="shim" directory="${stage}/shim" />
    <ivy-resolve conf="big-data-samples" directory="${stage}/big-data-samples" />
    <ivy-resolve conf="hive-jdbc" directory="${stage}/hive-jdbc" />
    <unzip dest="${run}">
      <fileset dir="${stage}/pdi-ce" />
      <mapper>
        <globmapper from="data-integration/*" to="*" />
      </mapper>
    </unzip>
    <unzip dest="${run}/plugins">
      <fileset dir="${stage}/big-data-plugin" />
    </unzip>
    <delete includeemptydirs="true">
      <fileset dir="${run}/plugins/pentaho-big-data-plugin/hadoop-configurations/" includes="**/*" />
    </delete>
    <unzip dest="${run}/plugins/pentaho-big-data-plugin/hadoop-configurations/">
      <fileset dir="${stage}/shim" />
    </unzip>
    <unzip dest="${run}/samples">
      <fileset dir="${stage}/big-data-samples" />
    </unzip>
    <replace file="${run}/plugins/pentaho-big-data-plugin/plugin.properties" token="hadoop-20" value="${shim}"/>
  </target>

  <target name="create-dot-classpath" depends="install-pentaho-ant-tasks,init">
    <dot-classpath>
      <!-- Include all lib dirs -->
      <classpath>
        <fileset dir="${testlib.dir}" includes="*.jar"/>
        <fileset dir="${lib.dir}">
          <include name="*.jar" />
        </fileset>
        <fileset dir="${devlib.dir}">
          <include name="*.jar" />
        </fileset>
        <fileset dir="${run-lib}">
          <include name="*.jar" />
        </fileset>
      </classpath>
    </dot-classpath>
  </target>

  <target name="clean-all" depends="subfloor.clean-all">
    <delete dir="${run}" />
  </target>

  <target name="compile-tests" depends="init-tests">
    <javac destdir="${testclasses.dir}"
           debug="true"
           optimize="false"
           source="${javac.source}"
           target="${javac.target}"
           fork="true">
      <src path="${testsrc.dir}" />
      <classpath>
        <fileset dir="${testlib.dir}" includes="*.jar"/>
        <fileset dir="${run-lib}" includes="kettle-core*.jar"/>
        <fileset dir="${run-lib}" includes="kettle-engine*.jar"/>
        <!-- <fileset dir="test/libext/" includes="kettle.jar"/>-->
        <pathelement path="${testclasses.dir}"/>

        <fileset dir="${run-lib}" 		includes="*.jar *.zip"/>
        <fileset dir="${run-libswt}" 		includes="*.jar *.zip"/>
        <fileset dir="${run-libswt}/win32/" includes="*.jar *.zip"/>
      </classpath>
    </javac>

    <!-- Copy the non-java files from the source directory to the test classes directory
-->
    <copy todir="${testclasses.dir}">
      <fileset dir="${testsrc.dir}">
        <exclude name="**/*.java" />
      </fileset>
    </copy>
  </target>

  <target name="test" depends="compile,compile-tests,init-test-reports" description="Execute the tests">
    <junit maxmemory="${junit.maxmemory}"
           fork="yes"
           failureProperty="test.failed"
           haltonfailure="${junit.haltonfailure}"
           printsummary="yes"
           forkmode="once"
           jvm="java"
           dir="${run}">
      <sysproperty key="hadoop.hostname" value="${hadoop.hostname}" />
      <sysproperty key="hadoop.port" value="${hadoop.port}" />
      <sysproperty key="hadoop.scheme" value="${hadoop.scheme}" />
      <sysproperty key="hive.database" value="${hive.database}" />
      <sysproperty key="hive.hostname" value="${hive.hostname}" />
      <sysproperty key="hive.port" value="${hive.port}" />
      <sysproperty key="hive.type" value="${hive.type}" />
      <sysproperty key="zookeeper.hostname" value="${zookeeper.hostname}" />
      <sysproperty key="zookeeper.port" value="${zookeeper.port}" />
      <sysproperty key="jobtracker.hostname" value="${jobtracker.hostname}" />
      <sysproperty key="jobtracker.port" value="${jobtracker.port}" />
      <sysproperty key="job.dirname" value="${jobs-full}" />
      <sysproperty key="hadoop.mapper" value="Hadoop-InputOutput-test.ktr" />
      <sysproperty key="job.order" value="load_wordcount.kjb" />
      <sysproperty key="pmr-sample.jar.location" value="samples/jobs/hadoop/pentaho-mapreduce-sample.jar" />

      <classpath>
        <fileset dir="${testlib.dir}" includes="*.jar"/>
        <fileset dir="${run-lib}" includes="kettle-core*.jar"/>
        <fileset dir="${run-lib}" includes="kettle-engine*.jar"/>
        <fileset dir="${run-lib}" includes="kettle-db*.jar"/>
        <pathelement path="${testclasses.dir}"/>

        <fileset dir="${run-lib}" 		includes="*.jar *.zip"/>
        <fileset dir="${run-libswt}" 		includes="*.jar *.zip"/>
        <fileset dir="${run-libswt}/win32/" includes="*.jar *.zip"/>
      </classpath>

      <sysproperty key="java.awt.headless" value="${headless.unittest}" />

      <syspropertyset>
        <propertyref prefix="junit.sysprop." />
        <mapper type="glob" from="junit.sysprop.*" to="*"/>
      </syspropertyset>
      <formatter type="xml" />

      <!-- <test name="${testcase}" todir="${testreports.xml.dir}" if="testcase" />-->
      <batchtest todir="${testreports.xml.dir}">
        <fileset dir="${testsrc.dir}" casesensitive="yes">
          <include name="**/*Test.java" />
        </fileset>
      </batchtest>
    </junit>

    <junitreport todir="${testreports.html.dir}">
      <fileset dir="${testreports.xml.dir}">
        <include name="TEST-*.xml" />
      </fileset>
      <report format="frames" todir="${testreports.html.dir}" />
    </junitreport>
  </target>

  <target name="load">
    <chmod file="${run}/kitchen.sh" perm="a+x" />
    <exec executable="./kitchen.sh" dir="${run}" failonerror="true">
      <arg value="-norep" />
      <arg value="-level Debug" />
      <arg value="-file" />
      <arg path="jobs/load_wordcount.kjb" />
      <arg value="-param:hadoop.hostname=${hadoop.hostname}" />
      <arg value="-param:hadoop.port=${hadoop.port}" />
      <arg value="-param:hadoop.scheme=${hadoop.scheme}" />
      <arg value="-param:hive.database=${hive.database}" />
      <arg value="-param:hive.hostname=${hive.hostname}" />
      <arg value="-param:hive.port=${hive.port}" />
      <arg value="-param:hive.type=${hive.type}" />
      <arg value="-param:zookeeper.hostname=${zookeeper.hostname}" />
      <arg value="-param:zookeeper.port=${zookeeper.port}" />
    </exec>
  </target>

  <target name="pig">
    <chmod file="${run}/kitchen.sh" perm="a+x" />
    <exec executable="./kitchen.sh" dir="${run}" failonerror="true">
      <arg value="-norep" />
      <arg value="-file" />
      <arg path="jobs/aggregate_pig.kjb" />
      <arg value="-param:hadoop.hostname=${hadoop.hostname}" />
      <arg value="-param:hadoop.port=${hadoop.port}" />
      <arg value="-param:hadoop.scheme=${hadoop.scheme}" />
      <arg value="-param:jobtracker.hostname=${jobtracker.hostname}" />
      <arg value="-param:jobtracker.port=${jobtracker.port}" />
    </exec>
  </target>

  <target name="mr">
    <chmod file="${run}/kitchen.sh" perm="a+x" />
    <exec executable="./kitchen.sh" dir="${run}" failonerror="true">
      <arg value="-norep" />
      <arg value="-file" />
      <arg path="jobs/aggregate_mr.kjb" />
      <arg value="-param:hadoop.hostname=${hadoop.hostname}" />
      <arg value="-param:hadoop.port=${hadoop.port}" />
      <arg value="-param:jobtracker.hostname=${jobtracker.hostname}" />
      <arg value="-param:jobtracker.port=${jobtracker.port}" />
    </exec>
  </target>

  <target name="pmr-wc">
    <chmod file="${run}/kitchen.sh" perm="a+x" />
    <exec executable="./kitchen.sh" dir="${run}" failonerror="true">
      <arg value="-norep" />
      <arg value="-file" />
      <arg path="jobs/Pentaho MapReduce - wordcount - test.kjb" />
      <arg value="-param:hadoop.hostname=${hadoop.hostname}" />
      <arg value="-param:hadoop.port=${hadoop.port}" />
      <arg value="-param:jobtracker.hostname=${jobtracker.hostname}" />
      <arg value="-param:jobtracker.port=${jobtracker.port}" />
    </exec>
  </target>

  <target name="pmr-hbase">
    <chmod file="${run}/kitchen.sh" perm="a+x" />
    <exec executable="./kitchen.sh" dir="${run}" failonerror="true">
      <arg value="-norep" />
      <arg value="-file" />
      <arg path="jobs/Pentaho MapReduce - HBase - test.kjb" />
      <arg value="-param:hadoop.hostname=${hadoop.hostname}" />
      <arg value="-param:hadoop.port=${hadoop.port}" />
      <arg value="-param:jobtracker.hostname=${jobtracker.hostname}" />
      <arg value="-param:jobtracker.port=${jobtracker.port}" />
      <arg value="-param:zookeeper.hostname=${zookeeper.hostname}" />
      <arg value="-param:zookeeper.port=${zookeeper.port}" />
    </exec>
  </target>

  <target name="pmr-simple">
    <chmod file="${run}/kitchen.sh" perm="a+x" />
    <exec executable="./kitchen.sh" dir="${run}" failonerror="true">
      <arg value="-norep" />
      <arg value="-file" />
      <arg path="jobs/Hadoop Job Executor simple.kjb" />
      <arg value="-param:hadoop.hostname=${hadoop.hostname}" />
      <arg value="-param:hadoop.port=${hadoop.port}" />
      <arg value="-param:hadoop.scheme=${hadoop.scheme}" />
      <arg value="-param:jobtracker.hostname=${jobtracker.hostname}" />
      <arg value="-param:jobtracker.port=${jobtracker.port}" />
      <arg value="-param:pmr-sample.jar.location=samples/jobs/hadoop/pentaho-mapreduce-sample.jar" />
    </exec>
  </target>

  <target name="pmr-hadoop-inputoutput">
    <chmod file="${run}/kitchen.sh" perm="a+x" />
    <exec executable="./kitchen.sh" dir="${run}" failonerror="true">
      <arg value="-norep" />
      <arg value="-file" />
      <arg path="jobs/Pentaho MapReduce - Hadoop - test.kjb" />
      <arg value="-param:hadoop.hostname=${hadoop.hostname}" />
      <arg value="-param:hadoop.port=${hadoop.port}" />
      <arg value="-param:hadoop.scheme=${hadoop.scheme}" />
      <arg value="-param:hadoop.mapper=Hadoop-InputOutput-test.ktr" />
      <arg value="-param:jobtracker.hostname=${jobtracker.hostname}" />
      <arg value="-param:jobtracker.port=${jobtracker.port}" />
    </exec>
  </target>

  <target name="hive-input">
    <chmod file="${run}/kitchen.sh" perm="a+x" />
    <exec executable="./kitchen.sh" dir="${run}" failonerror="true">
      <arg value="-norep" />
      <arg value="-file" />
      <arg path="jobs/Hive-Input-test.kjb" />
      <arg value="-param:hive.hostname=${hive.hostname}" />
      <arg value="-param:hive.port=${hive.port}" />
      <arg value="-param:hive.database=${hive.database}" />
      <arg value="-param:hive.type=${hive.type}" />
    </exec>
  </target>

 <target name="run-all" depends="load,pig,mr,pmr-wc,pmr-hbase,pmr-simple,pmr-hadoop-inputoutput,hive-input" />
</project>
